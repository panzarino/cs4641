{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.genfromtxt('data.csv', delimiter=',', dtype=int)\n",
    "cutoff = int(data.shape[0]*0.8)\n",
    "train_data = data[:cutoff]\n",
    "test_data = data[cutoff:]\n",
    "train_X = train_data[:,:-1].astype(float)\n",
    "train_Y = train_data[:,-1].reshape(-1,1)\n",
    "test_X = test_data[:,:-1].astype(float)\n",
    "test_Y = test_data[:,-1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9938    0.9849    0.9894      3917\n",
      "           1     0.9881    0.9951    0.9916      4927\n",
      "\n",
      "    accuracy                         0.9906      8844\n",
      "   macro avg     0.9910    0.9900    0.9905      8844\n",
      "weighted avg     0.9906    0.9906    0.9906      8844\n",
      "\n",
      "0.9906151062867481\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9477    0.9235    0.9355       981\n",
      "           1     0.9402    0.9593    0.9497      1230\n",
      "\n",
      "    accuracy                         0.9435      2211\n",
      "   macro avg     0.9440    0.9414    0.9426      2211\n",
      "weighted avg     0.9435    0.9435    0.9434      2211\n",
      "\n",
      "0.9434644957033017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "rf_initial = RandomForestClassifier()\n",
    "rf_initial.fit(train_X, train_Y.ravel())\n",
    "print('Train')\n",
    "rf_initial_predictions = rf_initial.predict(train_X)\n",
    "print(classification_report(train_Y, rf_initial_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, rf_initial_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "rf_initial_predictions_test = rf_initial.predict(test_X)\n",
    "print(classification_report(test_Y, rf_initial_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, rf_initial_predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 168 candidates, totalling 504 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 504 out of 504 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [None, 20, 40, 60, 80, 100],\n",
       "                         'min_samples_leaf': [1, 2],\n",
       "                         'min_samples_split': [2, 5],\n",
       "                         'n_estimators': [100, 400, 700, 1000, 1300, 1700,\n",
       "                                          2000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100, 400, 700, 1000, 1300, 1700, 2000],\n",
    "    'max_depth': [None, 20, 40, 60, 80, 100],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid = RandomForestClassifier()\n",
    "rf_grid_cv = GridSearchCV(estimator=rf_grid, param_grid=params, cv=3, verbose=1, n_jobs=-1)\n",
    "rf_grid_cv.fit(train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9920    0.9867    0.9894      3917\n",
      "           1     0.9895    0.9937    0.9916      4927\n",
      "\n",
      "    accuracy                         0.9906      8844\n",
      "   macro avg     0.9908    0.9902    0.9905      8844\n",
      "weighted avg     0.9906    0.9906    0.9906      8844\n",
      "\n",
      "0.9906151062867481\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9478    0.9256    0.9366       981\n",
      "           1     0.9417    0.9593    0.9505      1230\n",
      "\n",
      "    accuracy                         0.9444      2211\n",
      "   macro avg     0.9448    0.9425    0.9435      2211\n",
      "weighted avg     0.9444    0.9444    0.9443      2211\n",
      "\n",
      "0.9443690637720489\n",
      "\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "rf_grid_predictions = rf_grid_cv.predict(train_X)\n",
    "print(classification_report(train_Y, rf_grid_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, rf_grid_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "rf_grid_predictions_test = rf_grid_cv.predict(test_X)\n",
    "print(classification_report(test_Y, rf_grid_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, rf_grid_predictions_test))\n",
    "print()\n",
    "print(rf_grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [None, 120, 140, 160, 180, 200, 220, 240],\n",
       "                         'n_estimators': [500, 600, 700, 800, 900]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [500, 600, 700, 800, 900],\n",
    "    'max_depth': [None, 120, 140, 160, 180, 200, 220, 240]\n",
    "}\n",
    "\n",
    "rf_grid_2 = RandomForestClassifier()\n",
    "rf_grid_2_cv = GridSearchCV(estimator=rf_grid, param_grid=params, verbose=1, n_jobs=-1)\n",
    "rf_grid_2_cv.fit(train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9948    0.9839    0.9893      3917\n",
      "           1     0.9873    0.9959    0.9916      4927\n",
      "\n",
      "    accuracy                         0.9906      8844\n",
      "   macro avg     0.9911    0.9899    0.9905      8844\n",
      "weighted avg     0.9907    0.9906    0.9906      8844\n",
      "\n",
      "0.9906151062867481\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9459    0.9266    0.9361       981\n",
      "           1     0.9424    0.9577    0.9500      1230\n",
      "\n",
      "    accuracy                         0.9439      2211\n",
      "   macro avg     0.9441    0.9422    0.9431      2211\n",
      "weighted avg     0.9439    0.9439    0.9439      2211\n",
      "\n",
      "0.9439167797376753\n",
      "\n",
      "{'max_depth': None, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "rf_grid_2_predictions = rf_grid_2_cv.predict(train_X)\n",
    "print(classification_report(train_Y, rf_grid_2_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, rf_grid_2_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "rf_grid_2_predictions_test = rf_grid_2_cv.predict(test_X)\n",
    "print(classification_report(test_Y, rf_grid_2_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, rf_grid_2_predictions_test))\n",
    "print()\n",
    "print(rf_grid_2_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9605    0.9377    0.9490      3917\n",
      "           1     0.9514    0.9694    0.9603      4927\n",
      "\n",
      "    accuracy                         0.9553      8844\n",
      "   macro avg     0.9560    0.9535    0.9546      8844\n",
      "weighted avg     0.9554    0.9553    0.9553      8844\n",
      "\n",
      "0.9553369516056083\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9419    0.9093    0.9253       981\n",
      "           1     0.9296    0.9553    0.9423      1230\n",
      "\n",
      "    accuracy                         0.9349      2211\n",
      "   macro avg     0.9358    0.9323    0.9338      2211\n",
      "weighted avg     0.9351    0.9349    0.9347      2211\n",
      "\n",
      "0.9348710990502035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_initial = SVC()\n",
    "svm_initial.fit(train_X, train_Y.ravel())\n",
    "svm_initial_predictions = svm_initial.predict(test_X)\n",
    "print('Train')\n",
    "svm_initial_predictions = svm_initial.predict(train_X)\n",
    "print(classification_report(train_Y, svm_initial_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, svm_initial_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "svm_initial_predictions_test = svm_initial.predict(test_X)\n",
    "print(classification_report(test_Y, svm_initial_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, svm_initial_predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': ['auto', 'scale'],\n",
       "                         'kernel': ['rbf', 'poly', 'sigmoid'],\n",
       "                         'probability': [True, False]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['auto', 'scale'],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'probability': [True, False]\n",
    "}\n",
    "\n",
    "svm_grid = SVC()\n",
    "svm_grid_cv = GridSearchCV(estimator=svm_grid, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n",
    "svm_grid_cv.fit(train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9902    0.9778    0.9839      3917\n",
      "           1     0.9825    0.9923    0.9874      4927\n",
      "\n",
      "    accuracy                         0.9859      8844\n",
      "   macro avg     0.9863    0.9850    0.9857      8844\n",
      "weighted avg     0.9859    0.9859    0.9859      8844\n",
      "\n",
      "0.9858661239258254\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9138    0.9297    0.9217       981\n",
      "           1     0.9431    0.9301    0.9366      1230\n",
      "\n",
      "    accuracy                         0.9299      2211\n",
      "   macro avg     0.9285    0.9299    0.9291      2211\n",
      "weighted avg     0.9301    0.9299    0.9300      2211\n",
      "\n",
      "0.9298959746720941\n",
      "\n",
      "{'C': 100, 'gamma': 'scale', 'kernel': 'rbf', 'probability': True}\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "svm_grid_predictions = svm_grid_cv.predict(train_X)\n",
    "print(classification_report(train_Y, svm_grid_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, svm_grid_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "svm_grid_predictions_test = svm_grid_cv.predict(test_X)\n",
    "print(classification_report(test_Y, svm_grid_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, svm_grid_predictions_test))\n",
    "print()\n",
    "print(svm_grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9887    0.9847    0.9867      3917\n",
      "           1     0.9879    0.9911    0.9895      4927\n",
      "\n",
      "    accuracy                         0.9882      8844\n",
      "   macro avg     0.9883    0.9879    0.9881      8844\n",
      "weighted avg     0.9882    0.9882    0.9882      8844\n",
      "\n",
      "0.9882406151062868\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9234    0.9337    0.9285       981\n",
      "           1     0.9467    0.9382    0.9424      1230\n",
      "\n",
      "    accuracy                         0.9362      2211\n",
      "   macro avg     0.9350    0.9360    0.9355      2211\n",
      "weighted avg     0.9363    0.9362    0.9363      2211\n",
      "\n",
      "0.9362279511533242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_initial = MLPClassifier()\n",
    "nn_initial.fit(train_X, train_Y.ravel())\n",
    "nn_initial_predictions = nn_initial.predict(test_X)\n",
    "print('Train')\n",
    "nn_initial_predictions = nn_initial.predict(train_X)\n",
    "print(classification_report(train_Y, nn_initial_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, nn_initial_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "nn_initial_predictions_test = nn_initial.predict(test_X)\n",
    "print(classification_report(test_Y, nn_initial_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, nn_initial_predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_fun=15000,\n",
       "                                     max_iter=200, momentum=0.9,\n",
       "                                     n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_state...\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'activation': ['logistic', 'tanh', 'relu'],\n",
       "                         'alpha': [0.001, 0.0001, 1e-05],\n",
       "                         'hidden_layer_sizes': [(100, 100), (200, 100),\n",
       "                                                (200, 200)],\n",
       "                         'solver': ['lbfgs', 'adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(100, 100), (200, 100), (200, 200)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'alpha': [.001, .0001, .00001]\n",
    "}\n",
    "\n",
    "nn_grid = MLPClassifier()\n",
    "nn_grid_cv = GridSearchCV(estimator=nn_grid, param_grid=params, cv=3, verbose=1, n_jobs=-1)\n",
    "nn_grid_cv.fit(train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9977    0.9770    0.9872      3917\n",
      "           1     0.9820    0.9982    0.9900      4927\n",
      "\n",
      "    accuracy                         0.9888      8844\n",
      "   macro avg     0.9898    0.9876    0.9886      8844\n",
      "weighted avg     0.9889    0.9888    0.9888      8844\n",
      "\n",
      "0.9888059701492538\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9365    0.9174    0.9269       981\n",
      "           1     0.9352    0.9504    0.9427      1230\n",
      "\n",
      "    accuracy                         0.9358      2211\n",
      "   macro avg     0.9359    0.9339    0.9348      2211\n",
      "weighted avg     0.9358    0.9358    0.9357      2211\n",
      "\n",
      "0.9357756671189507\n",
      "\n",
      "{'activation': 'tanh', 'alpha': 1e-05, 'hidden_layer_sizes': (200, 200), 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "nn_grid_predictions = nn_grid_cv.predict(train_X)\n",
    "print(classification_report(train_Y, nn_grid_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, nn_grid_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "nn_grid_predictions_test = nn_grid_cv.predict(test_X)\n",
    "print(classification_report(test_Y, nn_grid_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, nn_grid_predictions_test))\n",
    "print()\n",
    "print(nn_grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_fun=15000,\n",
       "                                     max_iter=200, momentum=0.9,\n",
       "                                     n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_state...e, shuffle=True,\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh'], 'alpha': [0.01, 0.001, 0.0001],\n",
       "                         'hidden_layer_sizes': [(200, 200), (300, 200),\n",
       "                                                (300, 300), (300, 200, 100),\n",
       "                                                (200, 200, 200)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(200, 200), (300, 200), (300, 300), (300, 200, 100), (200, 200, 200)],\n",
    "    'activation': ['tanh'],\n",
    "    'alpha': [.01, .001, .0001]\n",
    "}\n",
    "\n",
    "nn_grid_2 = MLPClassifier()\n",
    "nn_grid_2_cv = GridSearchCV(estimator=nn_grid, param_grid=params, cv=3, verbose=1, n_jobs=-1)\n",
    "nn_grid_2_cv.fit(train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9895    0.9890    0.9893      3917\n",
      "           1     0.9913    0.9917    0.9915      4927\n",
      "\n",
      "    accuracy                         0.9905      8844\n",
      "   macro avg     0.9904    0.9904    0.9904      8844\n",
      "weighted avg     0.9905    0.9905    0.9905      8844\n",
      "\n",
      "0.9905020352781547\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9037    0.9470    0.9248       981\n",
      "           1     0.9560    0.9195    0.9374      1230\n",
      "\n",
      "    accuracy                         0.9317      2211\n",
      "   macro avg     0.9299    0.9333    0.9311      2211\n",
      "weighted avg     0.9328    0.9317    0.9318      2211\n",
      "\n",
      "0.9317051108095884\n",
      "\n",
      "{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (200, 200, 200)}\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "nn_grid_2_predictions = nn_grid_2_cv.predict(train_X)\n",
    "print(classification_report(train_Y, nn_grid_2_predictions, digits=4))\n",
    "print(accuracy_score(train_Y, nn_grid_2_predictions))\n",
    "print()\n",
    "print('Test')\n",
    "nn_grid_2_predictions_test = nn_grid_2_cv.predict(test_X)\n",
    "print(classification_report(test_Y, nn_grid_2_predictions_test, digits=4))\n",
    "print(accuracy_score(test_Y, nn_grid_2_predictions_test))\n",
    "print()\n",
    "print(nn_grid_2_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "0.7019712092395985\n",
      "Test\n",
      "0.6892029107685065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_X, train_Y.ravel())\n",
    "\n",
    "print('Train')\n",
    "print(lr.score(train_X, train_Y))\n",
    "print('Test')\n",
    "print(lr.score(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Mean 0.9678814561167502\n",
      "Random Forest SD 0.024989996315341143\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]SVM Mean 0.9525070319187966\n",
      "SVM SD 0.019123903273824107\n",
      "Iteration 1, loss = 0.24120465\n",
      "Iteration 2, loss = 0.18200925\n",
      "Iteration 3, loss = 0.17586293\n",
      "Iteration 4, loss = 0.17218059\n",
      "Iteration 5, loss = 0.16501604\n",
      "Iteration 6, loss = 0.15691085\n",
      "Iteration 7, loss = 0.14922795\n",
      "Iteration 8, loss = 0.13619629\n",
      "Iteration 9, loss = 0.12782733\n",
      "Iteration 10, loss = 0.11909419\n",
      "Iteration 11, loss = 0.11317883\n",
      "Iteration 12, loss = 0.10333327\n",
      "Iteration 13, loss = 0.10607770\n",
      "Iteration 14, loss = 0.09692916\n",
      "Iteration 15, loss = 0.08750523\n",
      "Iteration 16, loss = 0.08616495\n",
      "Iteration 17, loss = 0.08510363\n",
      "Iteration 18, loss = 0.07875078\n",
      "Iteration 19, loss = 0.07527974\n",
      "Iteration 20, loss = 0.07285223\n",
      "Iteration 21, loss = 0.07438090\n",
      "Iteration 22, loss = 0.06688607\n",
      "Iteration 23, loss = 0.06538665\n",
      "Iteration 24, loss = 0.05933328\n",
      "Iteration 25, loss = 0.06038160\n",
      "Iteration 26, loss = 0.05818119\n",
      "Iteration 27, loss = 0.05138115\n",
      "Iteration 28, loss = 0.05172734\n",
      "Iteration 29, loss = 0.04993133\n",
      "Iteration 30, loss = 0.05027254\n",
      "Iteration 31, loss = 0.04824221\n",
      "Iteration 32, loss = 0.04426235\n",
      "Iteration 33, loss = 0.04627542\n",
      "Iteration 34, loss = 0.04616348\n",
      "Iteration 35, loss = 0.04122155\n",
      "Iteration 36, loss = 0.04184439\n",
      "Iteration 37, loss = 0.03913312\n",
      "Iteration 38, loss = 0.03783890\n",
      "Iteration 39, loss = 0.04180144\n",
      "Iteration 40, loss = 0.03819265\n",
      "Iteration 41, loss = 0.03686697\n",
      "Iteration 42, loss = 0.03321281\n",
      "Iteration 43, loss = 0.03569766\n",
      "Iteration 44, loss = 0.03358074\n",
      "Iteration 45, loss = 0.03234526\n",
      "Iteration 46, loss = 0.03286439\n",
      "Iteration 47, loss = 0.03081083\n",
      "Iteration 48, loss = 0.03190494\n",
      "Iteration 49, loss = 0.03262302\n",
      "Iteration 50, loss = 0.03079410\n",
      "Iteration 51, loss = 0.02889635\n",
      "Iteration 52, loss = 0.03075169\n",
      "Iteration 53, loss = 0.03141598\n",
      "Iteration 54, loss = 0.02961222\n",
      "Iteration 55, loss = 0.02780456\n",
      "Iteration 56, loss = 0.02831110\n",
      "Iteration 57, loss = 0.02869161\n",
      "Iteration 58, loss = 0.02916994\n",
      "Iteration 59, loss = 0.02743324\n",
      "Iteration 60, loss = 0.02767700\n",
      "Iteration 61, loss = 0.02696927\n",
      "Iteration 62, loss = 0.02841370\n",
      "Iteration 63, loss = 0.02600874\n",
      "Iteration 64, loss = 0.02558251\n",
      "Iteration 65, loss = 0.02683456\n",
      "Iteration 66, loss = 0.02611441\n",
      "Iteration 67, loss = 0.02573259\n",
      "Iteration 68, loss = 0.02609171\n",
      "Iteration 69, loss = 0.02511448\n",
      "Iteration 70, loss = 0.02466488\n",
      "Iteration 71, loss = 0.02512819\n",
      "Iteration 72, loss = 0.02517764\n",
      "Iteration 73, loss = 0.02497625\n",
      "Iteration 74, loss = 0.02523481\n",
      "Iteration 75, loss = 0.02459763\n",
      "Iteration 76, loss = 0.02599622\n",
      "Iteration 77, loss = 0.02399035\n",
      "Iteration 78, loss = 0.02338350\n",
      "Iteration 79, loss = 0.02417102\n",
      "Iteration 80, loss = 0.02418175\n",
      "Iteration 81, loss = 0.02338366\n",
      "Iteration 82, loss = 0.02398369\n",
      "Iteration 83, loss = 0.02358756\n",
      "Iteration 84, loss = 0.02248530\n",
      "Iteration 85, loss = 0.02316195\n",
      "Iteration 86, loss = 0.02420286\n",
      "Iteration 87, loss = 0.02424740\n",
      "Iteration 88, loss = 0.02251995\n",
      "Iteration 89, loss = 0.02311086\n",
      "Iteration 90, loss = 0.02324039\n",
      "Iteration 91, loss = 0.02199421\n",
      "Iteration 92, loss = 0.02265211\n",
      "Iteration 93, loss = 0.02271256\n",
      "Iteration 94, loss = 0.02418374\n",
      "Iteration 95, loss = 0.02291107\n",
      "Iteration 96, loss = 0.02312187\n",
      "Iteration 97, loss = 0.02220410\n",
      "Iteration 98, loss = 0.02191405\n",
      "Iteration 99, loss = 0.02266348\n",
      "Iteration 100, loss = 0.02279777\n",
      "Iteration 101, loss = 0.02235954\n",
      "Iteration 102, loss = 0.02226477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33699092\n",
      "Iteration 2, loss = 0.17461233\n",
      "Iteration 3, loss = 0.15962892\n",
      "Iteration 4, loss = 0.13948852\n",
      "Iteration 5, loss = 0.12641055\n",
      "Iteration 6, loss = 0.12476335\n",
      "Iteration 7, loss = 0.12770293\n",
      "Iteration 8, loss = 0.11987644\n",
      "Iteration 9, loss = 0.11723152\n",
      "Iteration 10, loss = 0.11871306\n",
      "Iteration 11, loss = 0.12708811\n",
      "Iteration 12, loss = 0.12012954\n",
      "Iteration 13, loss = 0.11242573\n",
      "Iteration 14, loss = 0.12531912\n",
      "Iteration 15, loss = 0.11755480\n",
      "Iteration 16, loss = 0.11126786\n",
      "Iteration 17, loss = 0.10575138\n",
      "Iteration 18, loss = 0.10865964\n",
      "Iteration 19, loss = 0.10584170\n",
      "Iteration 20, loss = 0.11423526\n",
      "Iteration 21, loss = 0.12144212\n",
      "Iteration 22, loss = 0.10488488\n",
      "Iteration 23, loss = 0.09842354\n",
      "Iteration 24, loss = 0.09363246\n",
      "Iteration 25, loss = 0.09218593\n",
      "Iteration 26, loss = 0.09021613\n",
      "Iteration 27, loss = 0.08948730\n",
      "Iteration 28, loss = 0.08736070\n",
      "Iteration 29, loss = 0.08702167\n",
      "Iteration 30, loss = 0.08768179\n",
      "Iteration 31, loss = 0.08273554\n",
      "Iteration 32, loss = 0.07945933\n",
      "Iteration 33, loss = 0.07672988\n",
      "Iteration 34, loss = 0.07459301\n",
      "Iteration 35, loss = 0.07916358\n",
      "Iteration 36, loss = 0.07730990\n",
      "Iteration 37, loss = 0.07059091\n",
      "Iteration 38, loss = 0.06978484\n",
      "Iteration 39, loss = 0.06635934\n",
      "Iteration 40, loss = 0.06211910\n",
      "Iteration 41, loss = 0.05937997\n",
      "Iteration 42, loss = 0.06021423\n",
      "Iteration 43, loss = 0.05899259\n",
      "Iteration 44, loss = 0.06052089\n",
      "Iteration 45, loss = 0.05847651\n",
      "Iteration 46, loss = 0.05926941\n",
      "Iteration 47, loss = 0.05342130\n",
      "Iteration 48, loss = 0.05192123\n",
      "Iteration 49, loss = 0.05348030\n",
      "Iteration 50, loss = 0.05327512\n",
      "Iteration 51, loss = 0.04718901\n",
      "Iteration 52, loss = 0.04772340\n",
      "Iteration 53, loss = 0.04460113\n",
      "Iteration 54, loss = 0.04488090\n",
      "Iteration 55, loss = 0.04265192\n",
      "Iteration 56, loss = 0.04123950\n",
      "Iteration 57, loss = 0.03973094\n",
      "Iteration 58, loss = 0.03965147\n",
      "Iteration 59, loss = 0.03718031\n",
      "Iteration 60, loss = 0.03623583\n",
      "Iteration 61, loss = 0.03947484\n",
      "Iteration 62, loss = 0.03759042\n",
      "Iteration 63, loss = 0.03375593\n",
      "Iteration 64, loss = 0.03334125\n",
      "Iteration 65, loss = 0.03583190\n",
      "Iteration 66, loss = 0.03664930\n",
      "Iteration 67, loss = 0.03434182\n",
      "Iteration 68, loss = 0.03491505\n",
      "Iteration 69, loss = 0.03820852\n",
      "Iteration 70, loss = 0.03128874\n",
      "Iteration 71, loss = 0.03115014\n",
      "Iteration 72, loss = 0.02950380\n",
      "Iteration 73, loss = 0.02841173\n",
      "Iteration 74, loss = 0.03291143\n",
      "Iteration 75, loss = 0.03315449\n",
      "Iteration 76, loss = 0.03392050\n",
      "Iteration 77, loss = 0.04749959\n",
      "Iteration 78, loss = 0.03665178\n",
      "Iteration 79, loss = 0.03038923\n",
      "Iteration 80, loss = 0.02667654\n",
      "Iteration 81, loss = 0.03049435\n",
      "Iteration 82, loss = 0.02802235\n",
      "Iteration 83, loss = 0.02554013\n",
      "Iteration 84, loss = 0.02553806\n",
      "Iteration 85, loss = 0.02505177\n",
      "Iteration 86, loss = 0.02672554\n",
      "Iteration 87, loss = 0.02460444\n",
      "Iteration 88, loss = 0.02455472\n",
      "Iteration 89, loss = 0.02257367\n",
      "Iteration 90, loss = 0.02551049\n",
      "Iteration 91, loss = 0.02687960\n",
      "Iteration 92, loss = 0.02581329\n",
      "Iteration 93, loss = 0.02596588\n",
      "Iteration 94, loss = 0.02454601\n",
      "Iteration 95, loss = 0.02531971\n",
      "Iteration 96, loss = 0.02216877\n",
      "Iteration 97, loss = 0.02033867\n",
      "Iteration 98, loss = 0.02041000\n",
      "Iteration 99, loss = 0.02057239\n",
      "Iteration 100, loss = 0.02027838\n",
      "Iteration 101, loss = 0.01910880\n",
      "Iteration 102, loss = 0.02524449\n",
      "Iteration 103, loss = 0.02190153\n",
      "Iteration 104, loss = 0.02135171\n",
      "Iteration 105, loss = 0.02628788\n",
      "Iteration 106, loss = 0.02156666\n",
      "Iteration 107, loss = 0.02374731\n",
      "Iteration 108, loss = 0.02305626\n",
      "Iteration 109, loss = 0.01925303\n",
      "Iteration 110, loss = 0.01880004\n",
      "Iteration 111, loss = 0.01881693\n",
      "Iteration 112, loss = 0.01894659\n",
      "Iteration 113, loss = 0.01888930\n",
      "Iteration 114, loss = 0.01775757\n",
      "Iteration 115, loss = 0.01755219\n",
      "Iteration 116, loss = 0.01825208\n",
      "Iteration 117, loss = 0.02221459\n",
      "Iteration 118, loss = 0.02084297\n",
      "Iteration 119, loss = 0.01953215\n",
      "Iteration 120, loss = 0.01856066\n",
      "Iteration 121, loss = 0.01958307\n",
      "Iteration 122, loss = 0.01843134\n",
      "Iteration 123, loss = 0.01829475\n",
      "Iteration 124, loss = 0.02082083\n",
      "Iteration 125, loss = 0.01853109\n",
      "Iteration 126, loss = 0.01982500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38119213\n",
      "Iteration 2, loss = 0.16689012\n",
      "Iteration 3, loss = 0.14111506\n",
      "Iteration 4, loss = 0.13030180\n",
      "Iteration 5, loss = 0.12295757\n",
      "Iteration 6, loss = 0.12327543\n",
      "Iteration 7, loss = 0.12611869\n",
      "Iteration 8, loss = 0.13455739\n",
      "Iteration 9, loss = 0.12100316\n",
      "Iteration 10, loss = 0.11578570\n",
      "Iteration 11, loss = 0.11523912\n",
      "Iteration 12, loss = 0.11481194\n",
      "Iteration 13, loss = 0.11093620\n",
      "Iteration 14, loss = 0.11285622\n",
      "Iteration 15, loss = 0.11154175\n",
      "Iteration 16, loss = 0.10731747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.10611685\n",
      "Iteration 18, loss = 0.11255374\n",
      "Iteration 19, loss = 0.10389954\n",
      "Iteration 20, loss = 0.10219883\n",
      "Iteration 21, loss = 0.10162844\n",
      "Iteration 22, loss = 0.09714285\n",
      "Iteration 23, loss = 0.10494016\n",
      "Iteration 24, loss = 0.10470729\n",
      "Iteration 25, loss = 0.09783039\n",
      "Iteration 26, loss = 0.09602579\n",
      "Iteration 27, loss = 0.08921990\n",
      "Iteration 28, loss = 0.09558918\n",
      "Iteration 29, loss = 0.09274216\n",
      "Iteration 30, loss = 0.08296390\n",
      "Iteration 31, loss = 0.08313953\n",
      "Iteration 32, loss = 0.07967192\n",
      "Iteration 33, loss = 0.07793616\n",
      "Iteration 34, loss = 0.07703297\n",
      "Iteration 35, loss = 0.07851800\n",
      "Iteration 36, loss = 0.07664628\n",
      "Iteration 37, loss = 0.07351682\n",
      "Iteration 38, loss = 0.06830616\n",
      "Iteration 39, loss = 0.07010716\n",
      "Iteration 40, loss = 0.06473889\n",
      "Iteration 41, loss = 0.06199401\n",
      "Iteration 42, loss = 0.06201968\n",
      "Iteration 43, loss = 0.06258843\n",
      "Iteration 44, loss = 0.06163775\n",
      "Iteration 45, loss = 0.05465762\n",
      "Iteration 46, loss = 0.05544248\n",
      "Iteration 47, loss = 0.05914202\n",
      "Iteration 48, loss = 0.06091347\n",
      "Iteration 49, loss = 0.05980062\n",
      "Iteration 50, loss = 0.05603518\n",
      "Iteration 51, loss = 0.05889320\n",
      "Iteration 52, loss = 0.05656573\n",
      "Iteration 53, loss = 0.04905167\n",
      "Iteration 54, loss = 0.04725984\n",
      "Iteration 55, loss = 0.04565441\n",
      "Iteration 56, loss = 0.04665572\n",
      "Iteration 57, loss = 0.04238644\n",
      "Iteration 58, loss = 0.04603268\n",
      "Iteration 59, loss = 0.04328838\n",
      "Iteration 60, loss = 0.03867347\n",
      "Iteration 61, loss = 0.03797890\n",
      "Iteration 62, loss = 0.03714302\n",
      "Iteration 63, loss = 0.03649135\n",
      "Iteration 64, loss = 0.03835627\n",
      "Iteration 65, loss = 0.03660343\n",
      "Iteration 66, loss = 0.03651367\n",
      "Iteration 67, loss = 0.03560509\n",
      "Iteration 68, loss = 0.03551572\n",
      "Iteration 69, loss = 0.03281697\n",
      "Iteration 70, loss = 0.03287584\n",
      "Iteration 71, loss = 0.03295273\n",
      "Iteration 72, loss = 0.03136874\n",
      "Iteration 73, loss = 0.03109471\n",
      "Iteration 74, loss = 0.03247597\n",
      "Iteration 75, loss = 0.03654604\n",
      "Iteration 76, loss = 0.03156210\n",
      "Iteration 77, loss = 0.03324788\n",
      "Iteration 78, loss = 0.03540802\n",
      "Iteration 79, loss = 0.03364765\n",
      "Iteration 80, loss = 0.03567708\n",
      "Iteration 81, loss = 0.03026046\n",
      "Iteration 82, loss = 0.02763944\n",
      "Iteration 83, loss = 0.02809474\n",
      "Iteration 84, loss = 0.02796008\n",
      "Iteration 85, loss = 0.02712994\n",
      "Iteration 86, loss = 0.03005906\n",
      "Iteration 87, loss = 0.02447437\n",
      "Iteration 88, loss = 0.02291376\n",
      "Iteration 89, loss = 0.02346280\n",
      "Iteration 90, loss = 0.02235231\n",
      "Iteration 91, loss = 0.02120741\n",
      "Iteration 92, loss = 0.02466306\n",
      "Iteration 93, loss = 0.02303838\n",
      "Iteration 94, loss = 0.02172641\n",
      "Iteration 95, loss = 0.02052016\n",
      "Iteration 96, loss = 0.01951675\n",
      "Iteration 97, loss = 0.02142423\n",
      "Iteration 98, loss = 0.01966200\n",
      "Iteration 99, loss = 0.01912195\n",
      "Iteration 100, loss = 0.02121362\n",
      "Iteration 101, loss = 0.02151972\n",
      "Iteration 102, loss = 0.01985094\n",
      "Iteration 103, loss = 0.01985036\n",
      "Iteration 104, loss = 0.01890510\n",
      "Iteration 105, loss = 0.01891331\n",
      "Iteration 106, loss = 0.01918962\n",
      "Iteration 107, loss = 0.02043442\n",
      "Iteration 108, loss = 0.02130884\n",
      "Iteration 109, loss = 0.02434113\n",
      "Iteration 110, loss = 0.01942725\n",
      "Iteration 111, loss = 0.02231123\n",
      "Iteration 112, loss = 0.01707407\n",
      "Iteration 113, loss = 0.01700054\n",
      "Iteration 114, loss = 0.01708393\n",
      "Iteration 115, loss = 0.01941843\n",
      "Iteration 116, loss = 0.01881869\n",
      "Iteration 117, loss = 0.02229010\n",
      "Iteration 118, loss = 0.01882692\n",
      "Iteration 119, loss = 0.01564085\n",
      "Iteration 120, loss = 0.01541976\n",
      "Iteration 121, loss = 0.01437856\n",
      "Iteration 122, loss = 0.01439140\n",
      "Iteration 123, loss = 0.01381179\n",
      "Iteration 124, loss = 0.01378842\n",
      "Iteration 125, loss = 0.01351504\n",
      "Iteration 126, loss = 0.01450842\n",
      "Iteration 127, loss = 0.01510685\n",
      "Iteration 128, loss = 0.01441443\n",
      "Iteration 129, loss = 0.01475187\n",
      "Iteration 130, loss = 0.01581355\n",
      "Iteration 131, loss = 0.01466984\n",
      "Iteration 132, loss = 0.01506818\n",
      "Iteration 133, loss = 0.01384022\n",
      "Iteration 134, loss = 0.01479995\n",
      "Iteration 135, loss = 0.01592734\n",
      "Iteration 136, loss = 0.01278201\n",
      "Iteration 137, loss = 0.01394617\n",
      "Iteration 138, loss = 0.01225175\n",
      "Iteration 139, loss = 0.01344617\n",
      "Iteration 140, loss = 0.01522504\n",
      "Iteration 141, loss = 0.01412924\n",
      "Iteration 142, loss = 0.01351512\n",
      "Iteration 143, loss = 0.01661300\n",
      "Iteration 144, loss = 0.01353138\n",
      "Iteration 145, loss = 0.01185919\n",
      "Iteration 146, loss = 0.01278755\n",
      "Iteration 147, loss = 0.01298220\n",
      "Iteration 148, loss = 0.01266481\n",
      "Iteration 149, loss = 0.01483701\n",
      "Iteration 150, loss = 0.01226022\n",
      "Iteration 151, loss = 0.01345009\n",
      "Iteration 152, loss = 0.01121277\n",
      "Iteration 153, loss = 0.01098600\n",
      "Iteration 154, loss = 0.01167112\n",
      "Iteration 155, loss = 0.01288858\n",
      "Iteration 156, loss = 0.01401959\n",
      "Iteration 157, loss = 0.01627827\n",
      "Iteration 158, loss = 0.01268647\n",
      "Iteration 159, loss = 0.01431952\n",
      "Iteration 160, loss = 0.01270810\n",
      "Iteration 161, loss = 0.01114214\n",
      "Iteration 162, loss = 0.01241246\n",
      "Iteration 163, loss = 0.01117256\n",
      "Iteration 164, loss = 0.01344885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32244443\n",
      "Iteration 2, loss = 0.14837681\n",
      "Iteration 3, loss = 0.15455423\n",
      "Iteration 4, loss = 0.13983579\n",
      "Iteration 5, loss = 0.12552603\n",
      "Iteration 6, loss = 0.12232795\n",
      "Iteration 7, loss = 0.12606197\n",
      "Iteration 8, loss = 0.12790470\n",
      "Iteration 9, loss = 0.12188485\n",
      "Iteration 10, loss = 0.11721009\n",
      "Iteration 11, loss = 0.11818387\n",
      "Iteration 12, loss = 0.11497624\n",
      "Iteration 13, loss = 0.11687361\n",
      "Iteration 14, loss = 0.11490601\n",
      "Iteration 15, loss = 0.11031572\n",
      "Iteration 16, loss = 0.11059882\n",
      "Iteration 17, loss = 0.11345943\n",
      "Iteration 18, loss = 0.10905108\n",
      "Iteration 19, loss = 0.10804530\n",
      "Iteration 20, loss = 0.10444404\n",
      "Iteration 21, loss = 0.10160994\n",
      "Iteration 22, loss = 0.09914294\n",
      "Iteration 23, loss = 0.10471959\n",
      "Iteration 24, loss = 0.10249159\n",
      "Iteration 25, loss = 0.11135756\n",
      "Iteration 26, loss = 0.10537651\n",
      "Iteration 27, loss = 0.11415490\n",
      "Iteration 28, loss = 0.09284796\n",
      "Iteration 29, loss = 0.10013406\n",
      "Iteration 30, loss = 0.08814804\n",
      "Iteration 31, loss = 0.08850532\n",
      "Iteration 32, loss = 0.08753249\n",
      "Iteration 33, loss = 0.08417344\n",
      "Iteration 34, loss = 0.08267331\n",
      "Iteration 35, loss = 0.08187506\n",
      "Iteration 36, loss = 0.07944133\n",
      "Iteration 37, loss = 0.07666760\n",
      "Iteration 38, loss = 0.07750546\n",
      "Iteration 39, loss = 0.07202150\n",
      "Iteration 40, loss = 0.07067366\n",
      "Iteration 41, loss = 0.06928168\n",
      "Iteration 42, loss = 0.06811746\n",
      "Iteration 43, loss = 0.06827920\n",
      "Iteration 44, loss = 0.06428980\n",
      "Iteration 45, loss = 0.06318757\n",
      "Iteration 46, loss = 0.06311665\n",
      "Iteration 47, loss = 0.05956438\n",
      "Iteration 48, loss = 0.05942600\n",
      "Iteration 49, loss = 0.05703028\n",
      "Iteration 50, loss = 0.05876971\n",
      "Iteration 51, loss = 0.05788029\n",
      "Iteration 52, loss = 0.05782984\n",
      "Iteration 53, loss = 0.05006203\n",
      "Iteration 54, loss = 0.06112465\n",
      "Iteration 55, loss = 0.05600749\n",
      "Iteration 56, loss = 0.05174988\n",
      "Iteration 57, loss = 0.04965830\n",
      "Iteration 58, loss = 0.05560969\n",
      "Iteration 59, loss = 0.05402506\n",
      "Iteration 60, loss = 0.06224942\n",
      "Iteration 61, loss = 0.05157346\n",
      "Iteration 62, loss = 0.04644546\n",
      "Iteration 63, loss = 0.04686322\n",
      "Iteration 64, loss = 0.04261836\n",
      "Iteration 65, loss = 0.04053039\n",
      "Iteration 66, loss = 0.04361040\n",
      "Iteration 67, loss = 0.03997611\n",
      "Iteration 68, loss = 0.04237937\n",
      "Iteration 69, loss = 0.03858494\n",
      "Iteration 70, loss = 0.03757640\n",
      "Iteration 71, loss = 0.03756772\n",
      "Iteration 72, loss = 0.03514170\n",
      "Iteration 73, loss = 0.03298573\n",
      "Iteration 74, loss = 0.03208797\n",
      "Iteration 75, loss = 0.03538612\n",
      "Iteration 76, loss = 0.03431641\n",
      "Iteration 77, loss = 0.03705287\n",
      "Iteration 78, loss = 0.04004815\n",
      "Iteration 79, loss = 0.03251263\n",
      "Iteration 80, loss = 0.03260799\n",
      "Iteration 81, loss = 0.03755629\n",
      "Iteration 82, loss = 0.03285856\n",
      "Iteration 83, loss = 0.02814876\n",
      "Iteration 84, loss = 0.03190905\n",
      "Iteration 85, loss = 0.03212085\n",
      "Iteration 86, loss = 0.02620573\n",
      "Iteration 87, loss = 0.02959324\n",
      "Iteration 88, loss = 0.02614094\n",
      "Iteration 89, loss = 0.02749937\n",
      "Iteration 90, loss = 0.02555691\n",
      "Iteration 91, loss = 0.03095159\n",
      "Iteration 92, loss = 0.02507764\n",
      "Iteration 93, loss = 0.02794554\n",
      "Iteration 94, loss = 0.02629926\n",
      "Iteration 95, loss = 0.02472715\n",
      "Iteration 96, loss = 0.02406779\n",
      "Iteration 97, loss = 0.02369481\n",
      "Iteration 98, loss = 0.02276498\n",
      "Iteration 99, loss = 0.02394515\n",
      "Iteration 100, loss = 0.02117116\n",
      "Iteration 101, loss = 0.02197638\n",
      "Iteration 102, loss = 0.02152137\n",
      "Iteration 103, loss = 0.02293473\n",
      "Iteration 104, loss = 0.02439270\n",
      "Iteration 105, loss = 0.02211729\n",
      "Iteration 106, loss = 0.02441276\n",
      "Iteration 107, loss = 0.02799470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, loss = 0.02708571\n",
      "Iteration 109, loss = 0.02214026\n",
      "Iteration 110, loss = 0.02175039\n",
      "Iteration 111, loss = 0.02025218\n",
      "Iteration 112, loss = 0.01878002\n",
      "Iteration 113, loss = 0.02046732\n",
      "Iteration 114, loss = 0.01876284\n",
      "Iteration 115, loss = 0.02046773\n",
      "Iteration 116, loss = 0.01986953\n",
      "Iteration 117, loss = 0.01820139\n",
      "Iteration 118, loss = 0.01687713\n",
      "Iteration 119, loss = 0.01730120\n",
      "Iteration 120, loss = 0.01743862\n",
      "Iteration 121, loss = 0.01746716\n",
      "Iteration 122, loss = 0.01815842\n",
      "Iteration 123, loss = 0.02043171\n",
      "Iteration 124, loss = 0.01820549\n",
      "Iteration 125, loss = 0.02070932\n",
      "Iteration 126, loss = 0.01756339\n",
      "Iteration 127, loss = 0.01713947\n",
      "Iteration 128, loss = 0.01744007\n",
      "Iteration 129, loss = 0.01830797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39159015\n",
      "Iteration 2, loss = 0.17203303\n",
      "Iteration 3, loss = 0.13996124\n",
      "Iteration 4, loss = 0.12742032\n",
      "Iteration 5, loss = 0.12969323\n",
      "Iteration 6, loss = 0.12312905\n",
      "Iteration 7, loss = 0.11989935\n",
      "Iteration 8, loss = 0.12095282\n",
      "Iteration 9, loss = 0.12069971\n",
      "Iteration 10, loss = 0.11787663\n",
      "Iteration 11, loss = 0.12389206\n",
      "Iteration 12, loss = 0.11966408\n",
      "Iteration 13, loss = 0.11336415\n",
      "Iteration 14, loss = 0.12124926\n",
      "Iteration 15, loss = 0.10869147\n",
      "Iteration 16, loss = 0.10648007\n",
      "Iteration 17, loss = 0.10529316\n",
      "Iteration 18, loss = 0.10668556\n",
      "Iteration 19, loss = 0.10392521\n",
      "Iteration 20, loss = 0.10479475\n",
      "Iteration 21, loss = 0.10786214\n",
      "Iteration 22, loss = 0.10877462\n",
      "Iteration 23, loss = 0.11073737\n",
      "Iteration 24, loss = 0.10231999\n",
      "Iteration 25, loss = 0.09383171\n",
      "Iteration 26, loss = 0.09101715\n",
      "Iteration 27, loss = 0.08935857\n",
      "Iteration 28, loss = 0.08863516\n",
      "Iteration 29, loss = 0.08842443\n",
      "Iteration 30, loss = 0.08212581\n",
      "Iteration 31, loss = 0.08464394\n",
      "Iteration 32, loss = 0.08090340\n",
      "Iteration 33, loss = 0.07737944\n",
      "Iteration 34, loss = 0.07701427\n",
      "Iteration 35, loss = 0.07277641\n",
      "Iteration 36, loss = 0.07063309\n",
      "Iteration 37, loss = 0.07074971\n",
      "Iteration 38, loss = 0.06959643\n",
      "Iteration 39, loss = 0.07526642\n",
      "Iteration 40, loss = 0.08485546\n",
      "Iteration 41, loss = 0.07152477\n",
      "Iteration 42, loss = 0.06566391\n",
      "Iteration 43, loss = 0.06376829\n",
      "Iteration 44, loss = 0.06190662\n",
      "Iteration 45, loss = 0.05760437\n",
      "Iteration 46, loss = 0.05886359\n",
      "Iteration 47, loss = 0.05578598\n",
      "Iteration 48, loss = 0.05251469\n",
      "Iteration 49, loss = 0.05658315\n",
      "Iteration 50, loss = 0.05452434\n",
      "Iteration 51, loss = 0.05012369\n",
      "Iteration 52, loss = 0.04974578\n",
      "Iteration 53, loss = 0.04659740\n",
      "Iteration 54, loss = 0.04583861\n",
      "Iteration 55, loss = 0.04824437\n",
      "Iteration 56, loss = 0.04418174\n",
      "Iteration 57, loss = 0.04361432\n",
      "Iteration 58, loss = 0.04698736\n",
      "Iteration 59, loss = 0.04387988\n",
      "Iteration 60, loss = 0.04244252\n",
      "Iteration 61, loss = 0.04612295\n",
      "Iteration 62, loss = 0.03977798\n",
      "Iteration 63, loss = 0.04369526\n",
      "Iteration 64, loss = 0.03956087\n",
      "Iteration 65, loss = 0.03659984\n",
      "Iteration 66, loss = 0.03651297\n",
      "Iteration 67, loss = 0.03403389\n",
      "Iteration 68, loss = 0.03754596\n",
      "Iteration 69, loss = 0.03671341\n",
      "Iteration 70, loss = 0.03443226\n",
      "Iteration 71, loss = 0.03259648\n",
      "Iteration 72, loss = 0.03280461\n",
      "Iteration 73, loss = 0.03553406\n",
      "Iteration 74, loss = 0.03267910\n",
      "Iteration 75, loss = 0.03168529\n",
      "Iteration 76, loss = 0.02879581\n",
      "Iteration 77, loss = 0.03119964\n",
      "Iteration 78, loss = 0.03266421\n",
      "Iteration 79, loss = 0.03045850\n",
      "Iteration 80, loss = 0.02954381\n",
      "Iteration 81, loss = 0.03431218\n",
      "Iteration 82, loss = 0.03140520\n",
      "Iteration 83, loss = 0.02569364\n",
      "Iteration 84, loss = 0.02536029\n",
      "Iteration 85, loss = 0.02586403\n",
      "Iteration 86, loss = 0.02303128\n",
      "Iteration 87, loss = 0.02516650\n",
      "Iteration 88, loss = 0.02571138\n",
      "Iteration 89, loss = 0.02141278\n",
      "Iteration 90, loss = 0.02250572\n",
      "Iteration 91, loss = 0.02453713\n",
      "Iteration 92, loss = 0.02519444\n",
      "Iteration 93, loss = 0.02179292\n",
      "Iteration 94, loss = 0.02107214\n",
      "Iteration 95, loss = 0.02135515\n",
      "Iteration 96, loss = 0.02052945\n",
      "Iteration 97, loss = 0.02127652\n",
      "Iteration 98, loss = 0.02209395\n",
      "Iteration 99, loss = 0.02087988\n",
      "Iteration 100, loss = 0.02067882\n",
      "Iteration 101, loss = 0.02227611\n",
      "Iteration 102, loss = 0.02253238\n",
      "Iteration 103, loss = 0.02389763\n",
      "Iteration 104, loss = 0.02425922\n",
      "Iteration 105, loss = 0.02237849\n",
      "Iteration 106, loss = 0.02110173\n",
      "Iteration 107, loss = 0.01860800\n",
      "Iteration 108, loss = 0.02027063\n",
      "Iteration 109, loss = 0.01808686\n",
      "Iteration 110, loss = 0.01887072\n",
      "Iteration 111, loss = 0.01875164\n",
      "Iteration 112, loss = 0.01868052\n",
      "Iteration 113, loss = 0.02134262\n",
      "Iteration 114, loss = 0.01873666\n",
      "Iteration 115, loss = 0.01701643\n",
      "Iteration 116, loss = 0.01711132\n",
      "Iteration 117, loss = 0.02126362\n",
      "Iteration 118, loss = 0.02051482\n",
      "Iteration 119, loss = 0.02024105\n",
      "Iteration 120, loss = 0.01912311\n",
      "Iteration 121, loss = 0.01623549\n",
      "Iteration 122, loss = 0.01614616\n",
      "Iteration 123, loss = 0.01630223\n",
      "Iteration 124, loss = 0.01542709\n",
      "Iteration 125, loss = 0.01480724\n",
      "Iteration 126, loss = 0.01519828\n",
      "Iteration 127, loss = 0.01586279\n",
      "Iteration 128, loss = 0.01566385\n",
      "Iteration 129, loss = 0.01616015\n",
      "Iteration 130, loss = 0.01875974\n",
      "Iteration 131, loss = 0.01673383\n",
      "Iteration 132, loss = 0.01541458\n",
      "Iteration 133, loss = 0.01483059\n",
      "Iteration 134, loss = 0.01518592\n",
      "Iteration 135, loss = 0.01560872\n",
      "Iteration 136, loss = 0.01479737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34808244\n",
      "Iteration 2, loss = 0.14874384\n",
      "Iteration 3, loss = 0.13137005\n",
      "Iteration 4, loss = 0.12291037\n",
      "Iteration 5, loss = 0.11936055\n",
      "Iteration 6, loss = 0.11243648\n",
      "Iteration 7, loss = 0.11083833\n",
      "Iteration 8, loss = 0.10730101\n",
      "Iteration 9, loss = 0.10767498\n",
      "Iteration 10, loss = 0.10973434\n",
      "Iteration 11, loss = 0.10890074\n",
      "Iteration 12, loss = 0.10272638\n",
      "Iteration 13, loss = 0.10119399\n",
      "Iteration 14, loss = 0.09851320\n",
      "Iteration 15, loss = 0.10281930\n",
      "Iteration 16, loss = 0.10021572\n",
      "Iteration 17, loss = 0.09419991\n",
      "Iteration 18, loss = 0.09562667\n",
      "Iteration 19, loss = 0.09678061\n",
      "Iteration 20, loss = 0.09058967\n",
      "Iteration 21, loss = 0.08694418\n",
      "Iteration 22, loss = 0.08669025\n",
      "Iteration 23, loss = 0.08776634\n",
      "Iteration 24, loss = 0.08183098\n",
      "Iteration 25, loss = 0.09477387\n",
      "Iteration 26, loss = 0.09799200\n",
      "Iteration 27, loss = 0.10368337\n",
      "Iteration 28, loss = 0.08607136\n",
      "Iteration 29, loss = 0.08253270\n",
      "Iteration 30, loss = 0.07263078\n",
      "Iteration 31, loss = 0.07188068\n",
      "Iteration 32, loss = 0.06951351\n",
      "Iteration 33, loss = 0.07309660\n",
      "Iteration 34, loss = 0.07097740\n",
      "Iteration 35, loss = 0.06540424\n",
      "Iteration 36, loss = 0.06531232\n",
      "Iteration 37, loss = 0.06927238\n",
      "Iteration 38, loss = 0.06228795\n",
      "Iteration 39, loss = 0.05575103\n",
      "Iteration 40, loss = 0.05514537\n",
      "Iteration 41, loss = 0.05337374\n",
      "Iteration 42, loss = 0.05660587\n",
      "Iteration 43, loss = 0.05157142\n",
      "Iteration 44, loss = 0.05209525\n",
      "Iteration 45, loss = 0.05377095\n",
      "Iteration 46, loss = 0.04996890\n",
      "Iteration 47, loss = 0.04590979\n",
      "Iteration 48, loss = 0.04475779\n",
      "Iteration 49, loss = 0.04321084\n",
      "Iteration 50, loss = 0.04230000\n",
      "Iteration 51, loss = 0.04203731\n",
      "Iteration 52, loss = 0.04667018\n",
      "Iteration 53, loss = 0.04236086\n",
      "Iteration 54, loss = 0.04353994\n",
      "Iteration 55, loss = 0.04163477\n",
      "Iteration 56, loss = 0.04151438\n",
      "Iteration 57, loss = 0.03607474\n",
      "Iteration 58, loss = 0.03498419\n",
      "Iteration 59, loss = 0.03384598\n",
      "Iteration 60, loss = 0.03231282\n",
      "Iteration 61, loss = 0.03540568\n",
      "Iteration 62, loss = 0.03481049\n",
      "Iteration 63, loss = 0.03392664\n",
      "Iteration 64, loss = 0.03263507\n",
      "Iteration 65, loss = 0.03425417\n",
      "Iteration 66, loss = 0.03293021\n",
      "Iteration 67, loss = 0.02842117\n",
      "Iteration 68, loss = 0.02832965\n",
      "Iteration 69, loss = 0.02807070\n",
      "Iteration 70, loss = 0.02811253\n",
      "Iteration 71, loss = 0.02901667\n",
      "Iteration 72, loss = 0.02774860\n",
      "Iteration 73, loss = 0.02915790\n",
      "Iteration 74, loss = 0.03233996\n",
      "Iteration 75, loss = 0.02803923\n",
      "Iteration 76, loss = 0.02369631\n",
      "Iteration 77, loss = 0.02416620\n",
      "Iteration 78, loss = 0.02376620\n",
      "Iteration 79, loss = 0.02543307\n",
      "Iteration 80, loss = 0.02413104\n",
      "Iteration 81, loss = 0.02511378\n",
      "Iteration 82, loss = 0.02072373\n",
      "Iteration 83, loss = 0.02143946\n",
      "Iteration 84, loss = 0.02304771\n",
      "Iteration 85, loss = 0.02141211\n",
      "Iteration 86, loss = 0.02577421\n",
      "Iteration 87, loss = 0.02137906\n",
      "Iteration 88, loss = 0.02164949\n",
      "Iteration 89, loss = 0.02275639\n",
      "Iteration 90, loss = 0.02125901\n",
      "Iteration 91, loss = 0.01898038\n",
      "Iteration 92, loss = 0.02374041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 93, loss = 0.01930305\n",
      "Iteration 94, loss = 0.02402541\n",
      "Iteration 95, loss = 0.02084793\n",
      "Iteration 96, loss = 0.02217042\n",
      "Iteration 97, loss = 0.01802786\n",
      "Iteration 98, loss = 0.01801483\n",
      "Iteration 99, loss = 0.01835673\n",
      "Iteration 100, loss = 0.01763527\n",
      "Iteration 101, loss = 0.01721805\n",
      "Iteration 102, loss = 0.01632473\n",
      "Iteration 103, loss = 0.01934302\n",
      "Iteration 104, loss = 0.01747758\n",
      "Iteration 105, loss = 0.01775332\n",
      "Iteration 106, loss = 0.02190488\n",
      "Iteration 107, loss = 0.01968537\n",
      "Iteration 108, loss = 0.01708680\n",
      "Iteration 109, loss = 0.01554010\n",
      "Iteration 110, loss = 0.01948305\n",
      "Iteration 111, loss = 0.01703119\n",
      "Iteration 112, loss = 0.01950441\n",
      "Iteration 113, loss = 0.02544806\n",
      "Iteration 114, loss = 0.02034345\n",
      "Iteration 115, loss = 0.01924641\n",
      "Iteration 116, loss = 0.02235446\n",
      "Iteration 117, loss = 0.02662343\n",
      "Iteration 118, loss = 0.01859015\n",
      "Iteration 119, loss = 0.01611528\n",
      "Iteration 120, loss = 0.01516403\n",
      "Iteration 121, loss = 0.01652318\n",
      "Iteration 122, loss = 0.01730854\n",
      "Iteration 123, loss = 0.01747890\n",
      "Iteration 124, loss = 0.01503734\n",
      "Iteration 125, loss = 0.01500979\n",
      "Iteration 126, loss = 0.01460738\n",
      "Iteration 127, loss = 0.01469692\n",
      "Iteration 128, loss = 0.01647510\n",
      "Iteration 129, loss = 0.02005813\n",
      "Iteration 130, loss = 0.01889168\n",
      "Iteration 131, loss = 0.01645390\n",
      "Iteration 132, loss = 0.02007923\n",
      "Iteration 133, loss = 0.01571341\n",
      "Iteration 134, loss = 0.01444405\n",
      "Iteration 135, loss = 0.01410085\n",
      "Iteration 136, loss = 0.01432021\n",
      "Iteration 137, loss = 0.01515479\n",
      "Iteration 138, loss = 0.01579128\n",
      "Iteration 139, loss = 0.01709239\n",
      "Iteration 140, loss = 0.01604335\n",
      "Iteration 141, loss = 0.01364724\n",
      "Iteration 142, loss = 0.01473688\n",
      "Iteration 143, loss = 0.01493217\n",
      "Iteration 144, loss = 0.01639463\n",
      "Iteration 145, loss = 0.01593672\n",
      "Iteration 146, loss = 0.01699237\n",
      "Iteration 147, loss = 0.01616420\n",
      "Iteration 148, loss = 0.01366942\n",
      "Iteration 149, loss = 0.01564971\n",
      "Iteration 150, loss = 0.01262398\n",
      "Iteration 151, loss = 0.01254513\n",
      "Iteration 152, loss = 0.01244084\n",
      "Iteration 153, loss = 0.01272309\n",
      "Iteration 154, loss = 0.01344885\n",
      "Iteration 155, loss = 0.01242874\n",
      "Iteration 156, loss = 0.01287771\n",
      "Iteration 157, loss = 0.01328588\n",
      "Iteration 158, loss = 0.01210441\n",
      "Iteration 159, loss = 0.01585305\n",
      "Iteration 160, loss = 0.01537778\n",
      "Iteration 161, loss = 0.01475212\n",
      "Iteration 162, loss = 0.01504687\n",
      "Iteration 163, loss = 0.01269904\n",
      "Iteration 164, loss = 0.01445529\n",
      "Iteration 165, loss = 0.01420813\n",
      "Iteration 166, loss = 0.01360084\n",
      "Iteration 167, loss = 0.01389435\n",
      "Iteration 168, loss = 0.01200064\n",
      "Iteration 169, loss = 0.01431757\n",
      "Iteration 170, loss = 0.01541227\n",
      "Iteration 171, loss = 0.01239315\n",
      "Iteration 172, loss = 0.01500204\n",
      "Iteration 173, loss = 0.01315555\n",
      "Iteration 174, loss = 0.01220051\n",
      "Iteration 175, loss = 0.01344496\n",
      "Iteration 176, loss = 0.01247235\n",
      "Iteration 177, loss = 0.01210318\n",
      "Iteration 178, loss = 0.01300290\n",
      "Iteration 179, loss = 0.01174018\n",
      "Iteration 180, loss = 0.01277042\n",
      "Iteration 181, loss = 0.01223102\n",
      "Iteration 182, loss = 0.01270931\n",
      "Iteration 183, loss = 0.01324312\n",
      "Iteration 184, loss = 0.01371754\n",
      "Iteration 185, loss = 0.01375500\n",
      "Iteration 186, loss = 0.01402160\n",
      "Iteration 187, loss = 0.01572388\n",
      "Iteration 188, loss = 0.01487802\n",
      "Iteration 189, loss = 0.01411069\n",
      "Iteration 190, loss = 0.01275304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29458068\n",
      "Iteration 2, loss = 0.15423846\n",
      "Iteration 3, loss = 0.15656671\n",
      "Iteration 4, loss = 0.14197595\n",
      "Iteration 5, loss = 0.12354640\n",
      "Iteration 6, loss = 0.12057424\n",
      "Iteration 7, loss = 0.11660088\n",
      "Iteration 8, loss = 0.11833983\n",
      "Iteration 9, loss = 0.11762578\n",
      "Iteration 10, loss = 0.11728482\n",
      "Iteration 11, loss = 0.11542235\n",
      "Iteration 12, loss = 0.11473991\n",
      "Iteration 13, loss = 0.11158270\n",
      "Iteration 14, loss = 0.11010385\n",
      "Iteration 15, loss = 0.10657301\n",
      "Iteration 16, loss = 0.10288906\n",
      "Iteration 17, loss = 0.10230036\n",
      "Iteration 18, loss = 0.10005933\n",
      "Iteration 19, loss = 0.09804376\n",
      "Iteration 20, loss = 0.09824439\n",
      "Iteration 21, loss = 0.09517062\n",
      "Iteration 22, loss = 0.09728040\n",
      "Iteration 23, loss = 0.11112353\n",
      "Iteration 24, loss = 0.11777679\n",
      "Iteration 25, loss = 0.09665831\n",
      "Iteration 26, loss = 0.08862485\n",
      "Iteration 27, loss = 0.08816608\n",
      "Iteration 28, loss = 0.08532975\n",
      "Iteration 29, loss = 0.08364610\n",
      "Iteration 30, loss = 0.09380129\n",
      "Iteration 31, loss = 0.08442627\n",
      "Iteration 32, loss = 0.08036339\n",
      "Iteration 33, loss = 0.07943587\n",
      "Iteration 34, loss = 0.07861255\n",
      "Iteration 35, loss = 0.07621690\n",
      "Iteration 36, loss = 0.07423043\n",
      "Iteration 37, loss = 0.07286103\n",
      "Iteration 38, loss = 0.07229020\n",
      "Iteration 39, loss = 0.07465532\n",
      "Iteration 40, loss = 0.06890123\n",
      "Iteration 41, loss = 0.06471802\n",
      "Iteration 42, loss = 0.06417595\n",
      "Iteration 43, loss = 0.06024320\n",
      "Iteration 44, loss = 0.06001417\n",
      "Iteration 45, loss = 0.05702224\n",
      "Iteration 46, loss = 0.05882166\n",
      "Iteration 47, loss = 0.06787981\n",
      "Iteration 48, loss = 0.06091800\n",
      "Iteration 49, loss = 0.06017264\n",
      "Iteration 50, loss = 0.07403023\n",
      "Iteration 51, loss = 0.05629891\n",
      "Iteration 52, loss = 0.04885285\n",
      "Iteration 53, loss = 0.05073944\n",
      "Iteration 54, loss = 0.05077705\n",
      "Iteration 55, loss = 0.04629548\n",
      "Iteration 56, loss = 0.05274844\n",
      "Iteration 57, loss = 0.04499170\n",
      "Iteration 58, loss = 0.04373312\n",
      "Iteration 59, loss = 0.04207234\n",
      "Iteration 60, loss = 0.04310741\n",
      "Iteration 61, loss = 0.04080007\n",
      "Iteration 62, loss = 0.03777256\n",
      "Iteration 63, loss = 0.03787635\n",
      "Iteration 64, loss = 0.03858495\n",
      "Iteration 65, loss = 0.03573194\n",
      "Iteration 66, loss = 0.03932634\n",
      "Iteration 67, loss = 0.03695380\n",
      "Iteration 68, loss = 0.03256616\n",
      "Iteration 69, loss = 0.03256832\n",
      "Iteration 70, loss = 0.03459866\n",
      "Iteration 71, loss = 0.03430431\n",
      "Iteration 72, loss = 0.03621387\n",
      "Iteration 73, loss = 0.03234272\n",
      "Iteration 74, loss = 0.02949660\n",
      "Iteration 75, loss = 0.02891568\n",
      "Iteration 76, loss = 0.02832183\n",
      "Iteration 77, loss = 0.02686970\n",
      "Iteration 78, loss = 0.02828360\n",
      "Iteration 79, loss = 0.02503410\n",
      "Iteration 80, loss = 0.02727600\n",
      "Iteration 81, loss = 0.03243976\n",
      "Iteration 82, loss = 0.03350086\n",
      "Iteration 83, loss = 0.03217493\n",
      "Iteration 84, loss = 0.02512224\n",
      "Iteration 85, loss = 0.02532809\n",
      "Iteration 86, loss = 0.02611212\n",
      "Iteration 87, loss = 0.02682538\n",
      "Iteration 88, loss = 0.02591421\n",
      "Iteration 89, loss = 0.02805362\n",
      "Iteration 90, loss = 0.02286351\n",
      "Iteration 91, loss = 0.02363428\n",
      "Iteration 92, loss = 0.02265764\n",
      "Iteration 93, loss = 0.02191266\n",
      "Iteration 94, loss = 0.02116917\n",
      "Iteration 95, loss = 0.02018854\n",
      "Iteration 96, loss = 0.01945454\n",
      "Iteration 97, loss = 0.01963670\n",
      "Iteration 98, loss = 0.01860030\n",
      "Iteration 99, loss = 0.01786280\n",
      "Iteration 100, loss = 0.01835627\n",
      "Iteration 101, loss = 0.01876974\n",
      "Iteration 102, loss = 0.01805188\n",
      "Iteration 103, loss = 0.01965430\n",
      "Iteration 104, loss = 0.02030608\n",
      "Iteration 105, loss = 0.01984209\n",
      "Iteration 106, loss = 0.01815526\n",
      "Iteration 107, loss = 0.01817915\n",
      "Iteration 108, loss = 0.01916265\n",
      "Iteration 109, loss = 0.02229505\n",
      "Iteration 110, loss = 0.01997605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39125789\n",
      "Iteration 2, loss = 0.15787861\n",
      "Iteration 3, loss = 0.13952533\n",
      "Iteration 4, loss = 0.13308491\n",
      "Iteration 5, loss = 0.14130939\n",
      "Iteration 6, loss = 0.13401453\n",
      "Iteration 7, loss = 0.12668342\n",
      "Iteration 8, loss = 0.11825952\n",
      "Iteration 9, loss = 0.11664040\n",
      "Iteration 10, loss = 0.11553497\n",
      "Iteration 11, loss = 0.11486471\n",
      "Iteration 12, loss = 0.11644103\n",
      "Iteration 13, loss = 0.11597522\n",
      "Iteration 14, loss = 0.11595637\n",
      "Iteration 15, loss = 0.11336217\n",
      "Iteration 16, loss = 0.11523054\n",
      "Iteration 17, loss = 0.11051683\n",
      "Iteration 18, loss = 0.10518092\n",
      "Iteration 19, loss = 0.10462133\n",
      "Iteration 20, loss = 0.10097021\n",
      "Iteration 21, loss = 0.10352933\n",
      "Iteration 22, loss = 0.10692641\n",
      "Iteration 23, loss = 0.11193591\n",
      "Iteration 24, loss = 0.09526630\n",
      "Iteration 25, loss = 0.09312384\n",
      "Iteration 26, loss = 0.09643987\n",
      "Iteration 27, loss = 0.09541127\n",
      "Iteration 28, loss = 0.08907993\n",
      "Iteration 29, loss = 0.08941086\n",
      "Iteration 30, loss = 0.08495840\n",
      "Iteration 31, loss = 0.08144914\n",
      "Iteration 32, loss = 0.08380390\n",
      "Iteration 33, loss = 0.08226401\n",
      "Iteration 34, loss = 0.07914411\n",
      "Iteration 35, loss = 0.07573636\n",
      "Iteration 36, loss = 0.07342727\n",
      "Iteration 37, loss = 0.07120266\n",
      "Iteration 38, loss = 0.06903030\n",
      "Iteration 39, loss = 0.06821523\n",
      "Iteration 40, loss = 0.06705109\n",
      "Iteration 41, loss = 0.06584264\n",
      "Iteration 42, loss = 0.06308262\n",
      "Iteration 43, loss = 0.06320530\n",
      "Iteration 44, loss = 0.06014097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.05664210\n",
      "Iteration 46, loss = 0.05721118\n",
      "Iteration 47, loss = 0.05909870\n",
      "Iteration 48, loss = 0.05698755\n",
      "Iteration 49, loss = 0.05758044\n",
      "Iteration 50, loss = 0.05659822\n",
      "Iteration 51, loss = 0.05341665\n",
      "Iteration 52, loss = 0.04826338\n",
      "Iteration 53, loss = 0.04851421\n",
      "Iteration 54, loss = 0.05422943\n",
      "Iteration 55, loss = 0.05596786\n",
      "Iteration 56, loss = 0.05414824\n",
      "Iteration 57, loss = 0.05405505\n",
      "Iteration 58, loss = 0.05379818\n",
      "Iteration 59, loss = 0.04637641\n",
      "Iteration 60, loss = 0.04155693\n",
      "Iteration 61, loss = 0.04024007\n",
      "Iteration 62, loss = 0.03927691\n",
      "Iteration 63, loss = 0.03717538\n",
      "Iteration 64, loss = 0.03984653\n",
      "Iteration 65, loss = 0.03750131\n",
      "Iteration 66, loss = 0.03707636\n",
      "Iteration 67, loss = 0.03737438\n",
      "Iteration 68, loss = 0.03686825\n",
      "Iteration 69, loss = 0.03507335\n",
      "Iteration 70, loss = 0.03601950\n",
      "Iteration 71, loss = 0.03559694\n",
      "Iteration 72, loss = 0.03179008\n",
      "Iteration 73, loss = 0.03186026\n",
      "Iteration 74, loss = 0.03302766\n",
      "Iteration 75, loss = 0.03056642\n",
      "Iteration 76, loss = 0.03411336\n",
      "Iteration 77, loss = 0.03478883\n",
      "Iteration 78, loss = 0.02964788\n",
      "Iteration 79, loss = 0.03398926\n",
      "Iteration 80, loss = 0.03484813\n",
      "Iteration 81, loss = 0.03502295\n",
      "Iteration 82, loss = 0.03339079\n",
      "Iteration 83, loss = 0.03411244\n",
      "Iteration 84, loss = 0.03018150\n",
      "Iteration 85, loss = 0.03143206\n",
      "Iteration 86, loss = 0.03022843\n",
      "Iteration 87, loss = 0.02584582\n",
      "Iteration 88, loss = 0.02448453\n",
      "Iteration 89, loss = 0.02472773\n",
      "Iteration 90, loss = 0.02360155\n",
      "Iteration 91, loss = 0.02856174\n",
      "Iteration 92, loss = 0.02333875\n",
      "Iteration 93, loss = 0.02260043\n",
      "Iteration 94, loss = 0.02252265\n",
      "Iteration 95, loss = 0.02538797\n",
      "Iteration 96, loss = 0.02465151\n",
      "Iteration 97, loss = 0.02437294\n",
      "Iteration 98, loss = 0.02375427\n",
      "Iteration 99, loss = 0.02237347\n",
      "Iteration 100, loss = 0.02039579\n",
      "Iteration 101, loss = 0.02214115\n",
      "Iteration 102, loss = 0.02116110\n",
      "Iteration 103, loss = 0.02332656\n",
      "Iteration 104, loss = 0.02773827\n",
      "Iteration 105, loss = 0.02112520\n",
      "Iteration 106, loss = 0.02233025\n",
      "Iteration 107, loss = 0.02726319\n",
      "Iteration 108, loss = 0.02076588\n",
      "Iteration 109, loss = 0.02032970\n",
      "Iteration 110, loss = 0.01960765\n",
      "Iteration 111, loss = 0.01835047\n",
      "Iteration 112, loss = 0.01859020\n",
      "Iteration 113, loss = 0.01921316\n",
      "Iteration 114, loss = 0.01918563\n",
      "Iteration 115, loss = 0.02127291\n",
      "Iteration 116, loss = 0.02726606\n",
      "Iteration 117, loss = 0.02101236\n",
      "Iteration 118, loss = 0.02058668\n",
      "Iteration 119, loss = 0.01967359\n",
      "Iteration 120, loss = 0.01837056\n",
      "Iteration 121, loss = 0.01998597\n",
      "Iteration 122, loss = 0.01933219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37931706\n",
      "Iteration 2, loss = 0.14836646\n",
      "Iteration 3, loss = 0.13432599\n",
      "Iteration 4, loss = 0.13059298\n",
      "Iteration 5, loss = 0.13145735\n",
      "Iteration 6, loss = 0.12311127\n",
      "Iteration 7, loss = 0.11787265\n",
      "Iteration 8, loss = 0.11616013\n",
      "Iteration 9, loss = 0.11603636\n",
      "Iteration 10, loss = 0.11584586\n",
      "Iteration 11, loss = 0.11655176\n",
      "Iteration 12, loss = 0.11912301\n",
      "Iteration 13, loss = 0.11335826\n",
      "Iteration 14, loss = 0.10697288\n",
      "Iteration 15, loss = 0.10993656\n",
      "Iteration 16, loss = 0.10811741\n",
      "Iteration 17, loss = 0.11928885\n",
      "Iteration 18, loss = 0.11888750\n",
      "Iteration 19, loss = 0.10561113\n",
      "Iteration 20, loss = 0.10235919\n",
      "Iteration 21, loss = 0.10027982\n",
      "Iteration 22, loss = 0.09885927\n",
      "Iteration 23, loss = 0.09762687\n",
      "Iteration 24, loss = 0.09881153\n",
      "Iteration 25, loss = 0.09777593\n",
      "Iteration 26, loss = 0.09139543\n",
      "Iteration 27, loss = 0.08748076\n",
      "Iteration 28, loss = 0.08923183\n",
      "Iteration 29, loss = 0.09307453\n",
      "Iteration 30, loss = 0.08486245\n",
      "Iteration 31, loss = 0.08536926\n",
      "Iteration 32, loss = 0.08112873\n",
      "Iteration 33, loss = 0.08259741\n",
      "Iteration 34, loss = 0.07857807\n",
      "Iteration 35, loss = 0.07293899\n",
      "Iteration 36, loss = 0.07241897\n",
      "Iteration 37, loss = 0.07055392\n",
      "Iteration 38, loss = 0.07060320\n",
      "Iteration 39, loss = 0.06587642\n",
      "Iteration 40, loss = 0.06621787\n",
      "Iteration 41, loss = 0.06373604\n",
      "Iteration 42, loss = 0.06142027\n",
      "Iteration 43, loss = 0.06148859\n",
      "Iteration 44, loss = 0.05744454\n",
      "Iteration 45, loss = 0.05940018\n",
      "Iteration 46, loss = 0.05470500\n",
      "Iteration 47, loss = 0.05464027\n",
      "Iteration 48, loss = 0.05314204\n",
      "Iteration 49, loss = 0.06111746\n",
      "Iteration 50, loss = 0.05241930\n",
      "Iteration 51, loss = 0.04962070\n",
      "Iteration 52, loss = 0.05334943\n",
      "Iteration 53, loss = 0.04581411\n",
      "Iteration 54, loss = 0.04586067\n",
      "Iteration 55, loss = 0.04440326\n",
      "Iteration 56, loss = 0.05004252\n",
      "Iteration 57, loss = 0.06159342\n",
      "Iteration 58, loss = 0.04796884\n",
      "Iteration 59, loss = 0.04887604\n",
      "Iteration 60, loss = 0.04021102\n",
      "Iteration 61, loss = 0.03789932\n",
      "Iteration 62, loss = 0.03909613\n",
      "Iteration 63, loss = 0.03879112\n",
      "Iteration 64, loss = 0.03625148\n",
      "Iteration 65, loss = 0.03567265\n",
      "Iteration 66, loss = 0.04142183\n",
      "Iteration 67, loss = 0.03806507\n",
      "Iteration 68, loss = 0.03352228\n",
      "Iteration 69, loss = 0.03612731\n",
      "Iteration 70, loss = 0.03398067\n",
      "Iteration 71, loss = 0.03311022\n",
      "Iteration 72, loss = 0.03332027\n",
      "Iteration 73, loss = 0.03311826\n",
      "Iteration 74, loss = 0.02927317\n",
      "Iteration 75, loss = 0.03717082\n",
      "Iteration 76, loss = 0.03685618\n",
      "Iteration 77, loss = 0.03499941\n",
      "Iteration 78, loss = 0.03290276\n",
      "Iteration 79, loss = 0.02967316\n",
      "Iteration 80, loss = 0.02815305\n",
      "Iteration 81, loss = 0.02739942\n",
      "Iteration 82, loss = 0.02638412\n",
      "Iteration 83, loss = 0.02741591\n",
      "Iteration 84, loss = 0.02779932\n",
      "Iteration 85, loss = 0.02919879\n",
      "Iteration 86, loss = 0.03246651\n",
      "Iteration 87, loss = 0.03191224\n",
      "Iteration 88, loss = 0.03435052\n",
      "Iteration 89, loss = 0.03062685\n",
      "Iteration 90, loss = 0.02531867\n",
      "Iteration 91, loss = 0.02483411\n",
      "Iteration 92, loss = 0.02492015\n",
      "Iteration 93, loss = 0.02573655\n",
      "Iteration 94, loss = 0.02528726\n",
      "Iteration 95, loss = 0.02459222\n",
      "Iteration 96, loss = 0.02703639\n",
      "Iteration 97, loss = 0.02825401\n",
      "Iteration 98, loss = 0.02388233\n",
      "Iteration 99, loss = 0.02838494\n",
      "Iteration 100, loss = 0.02245409\n",
      "Iteration 101, loss = 0.02206128\n",
      "Iteration 102, loss = 0.02655050\n",
      "Iteration 103, loss = 0.02595417\n",
      "Iteration 104, loss = 0.02239790\n",
      "Iteration 105, loss = 0.02177176\n",
      "Iteration 106, loss = 0.02315357\n",
      "Iteration 107, loss = 0.02242251\n",
      "Iteration 108, loss = 0.02145788\n",
      "Iteration 109, loss = 0.02146650\n",
      "Iteration 110, loss = 0.01951210\n",
      "Iteration 111, loss = 0.01965627\n",
      "Iteration 112, loss = 0.01876071\n",
      "Iteration 113, loss = 0.02092383\n",
      "Iteration 114, loss = 0.02059712\n",
      "Iteration 115, loss = 0.01932317\n",
      "Iteration 116, loss = 0.01918382\n",
      "Iteration 117, loss = 0.01926035\n",
      "Iteration 118, loss = 0.02027972\n",
      "Iteration 119, loss = 0.01989366\n",
      "Iteration 120, loss = 0.01791973\n",
      "Iteration 121, loss = 0.02048322\n",
      "Iteration 122, loss = 0.02442561\n",
      "Iteration 123, loss = 0.02050281\n",
      "Iteration 124, loss = 0.02025649\n",
      "Iteration 125, loss = 0.01819005\n",
      "Iteration 126, loss = 0.01896509\n",
      "Iteration 127, loss = 0.01978777\n",
      "Iteration 128, loss = 0.01934525\n",
      "Iteration 129, loss = 0.02149317\n",
      "Iteration 130, loss = 0.02256395\n",
      "Iteration 131, loss = 0.02132047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29602761\n",
      "Iteration 2, loss = 0.15212854\n",
      "Iteration 3, loss = 0.13574967\n",
      "Iteration 4, loss = 0.13038432\n",
      "Iteration 5, loss = 0.13601421\n",
      "Iteration 6, loss = 0.12663344\n",
      "Iteration 7, loss = 0.12030180\n",
      "Iteration 8, loss = 0.11509616\n",
      "Iteration 9, loss = 0.11122902\n",
      "Iteration 10, loss = 0.11932930\n",
      "Iteration 11, loss = 0.12431761\n",
      "Iteration 12, loss = 0.12467874\n",
      "Iteration 13, loss = 0.12851221\n",
      "Iteration 14, loss = 0.11504539\n",
      "Iteration 15, loss = 0.11286140\n",
      "Iteration 16, loss = 0.10594030\n",
      "Iteration 17, loss = 0.10280667\n",
      "Iteration 18, loss = 0.10200839\n",
      "Iteration 19, loss = 0.10082010\n",
      "Iteration 20, loss = 0.10371115\n",
      "Iteration 21, loss = 0.10109059\n",
      "Iteration 22, loss = 0.09823700\n",
      "Iteration 23, loss = 0.09580746\n",
      "Iteration 24, loss = 0.09567955\n",
      "Iteration 25, loss = 0.09342981\n",
      "Iteration 26, loss = 0.08952720\n",
      "Iteration 27, loss = 0.09201968\n",
      "Iteration 28, loss = 0.08760622\n",
      "Iteration 29, loss = 0.08167490\n",
      "Iteration 30, loss = 0.08654135\n",
      "Iteration 31, loss = 0.07872631\n",
      "Iteration 32, loss = 0.07920427\n",
      "Iteration 33, loss = 0.07195390\n",
      "Iteration 34, loss = 0.06991949\n",
      "Iteration 35, loss = 0.07066507\n",
      "Iteration 36, loss = 0.06911030\n",
      "Iteration 37, loss = 0.06364987\n",
      "Iteration 38, loss = 0.06481911\n",
      "Iteration 39, loss = 0.06285430\n",
      "Iteration 40, loss = 0.06213325\n",
      "Iteration 41, loss = 0.06092388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.05842611\n",
      "Iteration 43, loss = 0.05222711\n",
      "Iteration 44, loss = 0.05190451\n",
      "Iteration 45, loss = 0.05101409\n",
      "Iteration 46, loss = 0.05395181\n",
      "Iteration 47, loss = 0.04932634\n",
      "Iteration 48, loss = 0.04733238\n",
      "Iteration 49, loss = 0.04745631\n",
      "Iteration 50, loss = 0.04809375\n",
      "Iteration 51, loss = 0.04870828\n",
      "Iteration 52, loss = 0.04752567\n",
      "Iteration 53, loss = 0.04551422\n",
      "Iteration 54, loss = 0.04054422\n",
      "Iteration 55, loss = 0.04161597\n",
      "Iteration 56, loss = 0.03886576\n",
      "Iteration 57, loss = 0.03711337\n",
      "Iteration 58, loss = 0.03614875\n",
      "Iteration 59, loss = 0.03435320\n",
      "Iteration 60, loss = 0.03497203\n",
      "Iteration 61, loss = 0.03605724\n",
      "Iteration 62, loss = 0.03184638\n",
      "Iteration 63, loss = 0.03188571\n",
      "Iteration 64, loss = 0.03150199\n",
      "Iteration 65, loss = 0.03090660\n",
      "Iteration 66, loss = 0.02955468\n",
      "Iteration 67, loss = 0.03443622\n",
      "Iteration 68, loss = 0.03689664\n",
      "Iteration 69, loss = 0.03857012\n",
      "Iteration 70, loss = 0.03484123\n",
      "Iteration 71, loss = 0.03095823\n",
      "Iteration 72, loss = 0.03046221\n",
      "Iteration 73, loss = 0.03353370\n",
      "Iteration 74, loss = 0.02909018\n",
      "Iteration 75, loss = 0.02834219\n",
      "Iteration 76, loss = 0.02583795\n",
      "Iteration 77, loss = 0.02380332\n",
      "Iteration 78, loss = 0.02741699\n",
      "Iteration 79, loss = 0.02519508\n",
      "Iteration 80, loss = 0.02361056\n",
      "Iteration 81, loss = 0.02296372\n",
      "Iteration 82, loss = 0.02479709\n",
      "Iteration 83, loss = 0.02781186\n",
      "Iteration 84, loss = 0.02931153\n",
      "Iteration 85, loss = 0.03764035\n",
      "Iteration 86, loss = 0.04523621\n",
      "Iteration 87, loss = 0.03718079\n",
      "Iteration 88, loss = 0.02318800\n",
      "Iteration 89, loss = 0.02028224\n",
      "Iteration 90, loss = 0.01973868\n",
      "Iteration 91, loss = 0.01901324\n",
      "Iteration 92, loss = 0.01952501\n",
      "Iteration 93, loss = 0.01942659\n",
      "Iteration 94, loss = 0.02239319\n",
      "Iteration 95, loss = 0.01956758\n",
      "Iteration 96, loss = 0.02112484\n",
      "Iteration 97, loss = 0.01766451\n",
      "Iteration 98, loss = 0.02160761\n",
      "Iteration 99, loss = 0.02011771\n",
      "Iteration 100, loss = 0.01779591\n",
      "Iteration 101, loss = 0.01757519\n",
      "Iteration 102, loss = 0.01770193\n",
      "Iteration 103, loss = 0.01989659\n",
      "Iteration 104, loss = 0.02046515\n",
      "Iteration 105, loss = 0.01813117\n",
      "Iteration 106, loss = 0.01785856\n",
      "Iteration 107, loss = 0.01629151\n",
      "Iteration 108, loss = 0.01759078\n",
      "Iteration 109, loss = 0.01668934\n",
      "Iteration 110, loss = 0.01674966\n",
      "Iteration 111, loss = 0.01688818\n",
      "Iteration 112, loss = 0.01884755\n",
      "Iteration 113, loss = 0.01843608\n",
      "Iteration 114, loss = 0.01592570\n",
      "Iteration 115, loss = 0.01549260\n",
      "Iteration 116, loss = 0.01541327\n",
      "Iteration 117, loss = 0.01683630\n",
      "Iteration 118, loss = 0.01861303\n",
      "Iteration 119, loss = 0.01643343\n",
      "Iteration 120, loss = 0.01675902\n",
      "Iteration 121, loss = 0.01656060\n",
      "Iteration 122, loss = 0.01565012\n",
      "Iteration 123, loss = 0.01576736\n",
      "Iteration 124, loss = 0.01793092\n",
      "Iteration 125, loss = 0.01763187\n",
      "Iteration 126, loss = 0.01902182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33882324\n",
      "Iteration 2, loss = 0.16292858\n",
      "Iteration 3, loss = 0.14744889\n",
      "Iteration 4, loss = 0.13669493\n",
      "Iteration 5, loss = 0.12913569\n",
      "Iteration 6, loss = 0.12339030\n",
      "Iteration 7, loss = 0.11924548\n",
      "Iteration 8, loss = 0.12581595\n",
      "Iteration 9, loss = 0.11940062\n",
      "Iteration 10, loss = 0.11486114\n",
      "Iteration 11, loss = 0.11269670\n",
      "Iteration 12, loss = 0.11205690\n",
      "Iteration 13, loss = 0.11566071\n",
      "Iteration 14, loss = 0.11710471\n",
      "Iteration 15, loss = 0.10962704\n",
      "Iteration 16, loss = 0.10758791\n",
      "Iteration 17, loss = 0.10900541\n",
      "Iteration 18, loss = 0.11033072\n",
      "Iteration 19, loss = 0.10486965\n",
      "Iteration 20, loss = 0.11044847\n",
      "Iteration 21, loss = 0.11424599\n",
      "Iteration 22, loss = 0.11209363\n",
      "Iteration 23, loss = 0.10234369\n",
      "Iteration 24, loss = 0.10109501\n",
      "Iteration 25, loss = 0.09688899\n",
      "Iteration 26, loss = 0.09746767\n",
      "Iteration 27, loss = 0.09227156\n",
      "Iteration 28, loss = 0.09010727\n",
      "Iteration 29, loss = 0.08842773\n",
      "Iteration 30, loss = 0.08669394\n",
      "Iteration 31, loss = 0.08682996\n",
      "Iteration 32, loss = 0.08774121\n",
      "Iteration 33, loss = 0.08713390\n",
      "Iteration 34, loss = 0.08372152\n",
      "Iteration 35, loss = 0.08138203\n",
      "Iteration 36, loss = 0.07927693\n",
      "Iteration 37, loss = 0.07743411\n",
      "Iteration 38, loss = 0.07374977\n",
      "Iteration 39, loss = 0.07113463\n",
      "Iteration 40, loss = 0.07070480\n",
      "Iteration 41, loss = 0.07041653\n",
      "Iteration 42, loss = 0.06808410\n",
      "Iteration 43, loss = 0.06988337\n",
      "Iteration 44, loss = 0.06707857\n",
      "Iteration 45, loss = 0.06720446\n",
      "Iteration 46, loss = 0.06283551\n",
      "Iteration 47, loss = 0.06031521\n",
      "Iteration 48, loss = 0.05989521\n",
      "Iteration 49, loss = 0.06167676\n",
      "Iteration 50, loss = 0.06875466\n",
      "Iteration 51, loss = 0.06884972\n",
      "Iteration 52, loss = 0.05659508\n",
      "Iteration 53, loss = 0.05208566\n",
      "Iteration 54, loss = 0.05021135\n",
      "Iteration 55, loss = 0.05256036\n",
      "Iteration 56, loss = 0.05235536\n",
      "Iteration 57, loss = 0.05633678\n",
      "Iteration 58, loss = 0.04950088\n",
      "Iteration 59, loss = 0.05426482\n",
      "Iteration 60, loss = 0.04781130\n",
      "Iteration 61, loss = 0.05260089\n",
      "Iteration 62, loss = 0.04589390\n",
      "Iteration 63, loss = 0.04820879\n",
      "Iteration 64, loss = 0.04410407\n",
      "Iteration 65, loss = 0.05316479\n",
      "Iteration 66, loss = 0.05296928\n",
      "Iteration 67, loss = 0.04359411\n",
      "Iteration 68, loss = 0.04338863\n",
      "Iteration 69, loss = 0.04139095\n",
      "Iteration 70, loss = 0.03693166\n",
      "Iteration 71, loss = 0.03485575\n",
      "Iteration 72, loss = 0.03623082\n",
      "Iteration 73, loss = 0.03735781\n",
      "Iteration 74, loss = 0.03580510\n",
      "Iteration 75, loss = 0.03405492\n",
      "Iteration 76, loss = 0.03346357\n",
      "Iteration 77, loss = 0.03217553\n",
      "Iteration 78, loss = 0.03222460\n",
      "Iteration 79, loss = 0.03303573\n",
      "Iteration 80, loss = 0.03319847\n",
      "Iteration 81, loss = 0.03316228\n",
      "Iteration 82, loss = 0.03721542\n",
      "Iteration 83, loss = 0.03102567\n",
      "Iteration 84, loss = 0.02949625\n",
      "Iteration 85, loss = 0.02886100\n",
      "Iteration 86, loss = 0.03148822\n",
      "Iteration 87, loss = 0.02829065\n",
      "Iteration 88, loss = 0.02746372\n",
      "Iteration 89, loss = 0.03186623\n",
      "Iteration 90, loss = 0.02873735\n",
      "Iteration 91, loss = 0.02842764\n",
      "Iteration 92, loss = 0.02661333\n",
      "Iteration 93, loss = 0.03052309\n",
      "Iteration 94, loss = 0.02958580\n",
      "Iteration 95, loss = 0.02681111\n",
      "Iteration 96, loss = 0.03518753\n",
      "Iteration 97, loss = 0.03335473\n",
      "Iteration 98, loss = 0.02766159\n",
      "Iteration 99, loss = 0.02826229\n",
      "Iteration 100, loss = 0.02764845\n",
      "Iteration 101, loss = 0.02643060\n",
      "Iteration 102, loss = 0.02503797\n",
      "Iteration 103, loss = 0.02443880\n",
      "Iteration 104, loss = 0.02268112\n",
      "Iteration 105, loss = 0.02483465\n",
      "Iteration 106, loss = 0.02328666\n",
      "Iteration 107, loss = 0.02265019\n",
      "Iteration 108, loss = 0.02162965\n",
      "Iteration 109, loss = 0.02325504\n",
      "Iteration 110, loss = 0.02052351\n",
      "Iteration 111, loss = 0.02037490\n",
      "Iteration 112, loss = 0.02205362\n",
      "Iteration 113, loss = 0.01983806\n",
      "Iteration 114, loss = 0.02046916\n",
      "Iteration 115, loss = 0.01947085\n",
      "Iteration 116, loss = 0.02175707\n",
      "Iteration 117, loss = 0.02323132\n",
      "Iteration 118, loss = 0.02882636\n",
      "Iteration 119, loss = 0.02635218\n",
      "Iteration 120, loss = 0.02216550\n",
      "Iteration 121, loss = 0.02560451\n",
      "Iteration 122, loss = 0.02637206\n",
      "Iteration 123, loss = 0.02556756\n",
      "Iteration 124, loss = 0.02664606\n",
      "Iteration 125, loss = 0.02201942\n",
      "Iteration 126, loss = 0.01913811\n",
      "Iteration 127, loss = 0.01863809\n",
      "Iteration 128, loss = 0.01876947\n",
      "Iteration 129, loss = 0.01956513\n",
      "Iteration 130, loss = 0.02153920\n",
      "Iteration 131, loss = 0.02103747\n",
      "Iteration 132, loss = 0.01989391\n",
      "Iteration 133, loss = 0.02167386\n",
      "Iteration 134, loss = 0.02144197\n",
      "Iteration 135, loss = 0.01863778\n",
      "Iteration 136, loss = 0.01937250\n",
      "Iteration 137, loss = 0.02168421\n",
      "Iteration 138, loss = 0.01737515\n",
      "Iteration 139, loss = 0.01744987\n",
      "Iteration 140, loss = 0.01844015\n",
      "Iteration 141, loss = 0.01743168\n",
      "Iteration 142, loss = 0.01710359\n",
      "Iteration 143, loss = 0.01750896\n",
      "Iteration 144, loss = 0.01744527\n",
      "Iteration 145, loss = 0.01940149\n",
      "Iteration 146, loss = 0.02015677\n",
      "Iteration 147, loss = 0.01683630\n",
      "Iteration 148, loss = 0.01784820\n",
      "Iteration 149, loss = 0.01722661\n",
      "Iteration 150, loss = 0.01710405\n",
      "Iteration 151, loss = 0.01867200\n",
      "Iteration 152, loss = 0.02125193\n",
      "Iteration 153, loss = 0.01817284\n",
      "Iteration 154, loss = 0.01777563\n",
      "Iteration 155, loss = 0.01621565\n",
      "Iteration 156, loss = 0.01620470\n",
      "Iteration 157, loss = 0.01731106\n",
      "Iteration 158, loss = 0.01602830\n",
      "Iteration 159, loss = 0.01635991\n",
      "Iteration 160, loss = 0.01683840\n",
      "Iteration 161, loss = 0.01780237\n",
      "Iteration 162, loss = 0.01685499\n",
      "Iteration 163, loss = 0.01635163\n",
      "Iteration 164, loss = 0.01596965\n",
      "Iteration 165, loss = 0.01595442\n",
      "Iteration 166, loss = 0.01549172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 167, loss = 0.01569491\n",
      "Iteration 168, loss = 0.01586179\n",
      "Iteration 169, loss = 0.01514759\n",
      "Iteration 170, loss = 0.01555517\n",
      "Iteration 171, loss = 0.01602015\n",
      "Iteration 172, loss = 0.01522127\n",
      "Iteration 173, loss = 0.01569252\n",
      "Iteration 174, loss = 0.02113494\n",
      "Iteration 175, loss = 0.01701095\n",
      "Iteration 176, loss = 0.01618638\n",
      "Iteration 177, loss = 0.01701705\n",
      "Iteration 178, loss = 0.01755862\n",
      "Iteration 179, loss = 0.01766345\n",
      "Iteration 180, loss = 0.01902881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Neural Network Mean 0.9619991031755738\n",
      "Neural Network SD 0.02115397201514253\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [x for x in range(1, 11)]\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=None, n_estimators=700, verbose=1)\n",
    "rf.fit(train_X, train_Y.ravel())\n",
    "plot_precision_recall_curve(rf, test_X, test_Y.ravel())\n",
    "\n",
    "rf_kfold = KFold(n_splits=10)\n",
    "rf_results = cross_val_score(rf, test_X, test_Y.ravel(), cv=rf_kfold, scoring='accuracy')\n",
    "print('Random Forest Mean', rf_results.mean())\n",
    "print('Random Forest SD', rf_results.std())\n",
    "\n",
    "svm = SVC(C=100, gamma='scale', kernel='rbf', probability=True, verbose=1)\n",
    "svm.fit(train_X, train_Y.ravel())\n",
    "plot_precision_recall_curve(svm, test_X, test_Y.ravel())\n",
    "\n",
    "svm_kfold = KFold(n_splits=10)\n",
    "svm_results = cross_val_score(svm, test_X, test_Y.ravel(), cv=svm_kfold, scoring='accuracy')\n",
    "print('SVM Mean', svm_results.mean())\n",
    "print('SVM SD', svm_results.std())\n",
    "\n",
    "nn = MLPClassifier(activation='tanh', alpha=0.0001, hidden_layer_sizes=(200, 200, 200), verbose=1)\n",
    "nn.fit(train_X, train_Y.ravel())\n",
    "plot_precision_recall_curve(nn, test_X, test_Y.ravel())\n",
    "\n",
    "nn_kfold = KFold(n_splits=10)\n",
    "nn_results = cross_val_score(nn, test_X, test_Y.ravel(), cv=nn_kfold, scoring='accuracy')\n",
    "print('Neural Network Mean', nn_results.mean())\n",
    "print('Neural Network SD', nn_results.std())\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(labels, rf_results, color='blue')\n",
    "plt.plot(labels, svd_results, color='red')\n",
    "plt.plot(labels, nn_results, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
